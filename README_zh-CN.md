# LLaVA-NeXT-Reproduced

[English](README.md) | ç®€ä½“ä¸­æ–‡

---

ç”±äºæˆªæ­¢åˆ°ç›®å‰ä¸ºæ­¢ (2024.07.06) å®˜æ–¹è¿˜æ²¡æœ‰å…¬å¼€ LLaVA-NeXT çš„è®­ç»ƒä»£ç å’Œè„šæœ¬ï¼Œè¿™ä¸ªä»“åº“æ—¨åœ¨å¤ç° LLaVA-NeXT çš„è®­ç»ƒä»£ç å’Œè„šæœ¬ (ä¸»è¦ç”¨äºä¸ªäººå­¦ä¹ )ã€‚

æˆ‘ä»¬å¼•å…¥ [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT) ç›®å‰æä¾›çš„ä»£ç  (åŒ…æ‹¬LLaVA-NeXTæ¨¡å‹å’Œç›¸å…³çš„æ•°æ®å¤„ç†å‡½æ•°)ï¼Œ å¹¶å‚è€ƒä¹‹å‰ç‰ˆæœ¬ ([LLaVA-v1.5](https://github.com/haotian-liu/LLaVA)) ä»¥åŠçš„è®­ç»ƒä»£ç å’Œè„šæœ¬ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„æ”¹åŠ¨æ¥å¤ç° LLaVA-NeXT çš„è®­ç»ƒã€‚

> åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥å°† [LLaVA-v1.5](https://github.com/haotian-liu/LLaVA) çš„è®­ç»ƒä»£ç å¤åˆ¶è¿‡æ¥å¹¶è¿›è¡Œæ›´æ”¹ã€‚å…·ä½“æ¥è¯´ï¼Œåªæœ‰ `llava/train/train.py` å’Œ `llava/train/llava_trainer.py` è¢«æ›´æ”¹ï¼Œä¸”æ›´æ”¹çš„éƒ¨åˆ†å‡æœ‰æ³¨é‡Šï¼Œä»¥ä¾¿è¯»è€…äº†è§£è¿™ä¸ªä»£ç ä¸LLaVA-v1.5çš„ä»£ç çš„åŒºåˆ«ã€‚  
> 
> å¯¹äºè®­ç»ƒè„šæœ¬ï¼Œæˆ‘ä»¬æ·»åŠ çš„è„šæœ¬åœ¨ `scripts/next`ï¼Œç›®å‰æä¾›äº† vicuna-v1.5-7b å’Œ llama3-8b ä¸º LLM åŸºåº§çš„è®­ç»ƒè„šæœ¬ï¼Œæœªæ¥æˆ‘ä»¬å°†è¡¥å……å…¶å®ƒæ¨¡å‹çš„è„šæœ¬ã€‚

## â³ ToDo

- [X] Reproduce LLaVA-Next-Vicuna-7B
- [X] Reproduce LLaVA-Next-LLaMA3-8B
- [ ] Reproduce LLaVA-Next-Qwen2-7B
- [ ] Support for SigLIP as the vision tower

## ğŸ”§ å®‰è£…

### 1. **å…‹éš†ä»“åº“å¹¶è¿›å…¥ LLaVA-NeXT æ–‡ä»¶å¤¹:**
```bash
git clone https://github.com/LLaVA-VL/LLaVA-NeXT
cd LLaVA-NeXT
```

### 2. **å®‰è£…ç¯å¢ƒ:**
```bash
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # Enable PEP 660 support.
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

## ğŸ“ æ•°æ®å‡†å¤‡

æ‚¨å¯ä»¥æŒ‰ç…§ [Data.md](docs/Data.md) è·å–æ•°æ®é›†ã€‚

## ğŸš† è®­ç»ƒ

### è¶…å‚æ•°

åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸ Vicuna ç±»ä¼¼çš„ä¸€ç»„è¶…å‚æ•°ã€‚ ä¸‹é¢æä¾›äº†é¢„è®­ç»ƒå’Œå¾®è°ƒä¸­ä½¿ç”¨çš„ä¸¤ä¸ªè¶…å‚æ•°ã€‚

1. Pretraining

| Hyperparameter       | Global Batch Size | Projector lr | Epochs | Max length | Weight decay |
| -------------------- | ----------------: | -----------: | -----: | ---------: | -----------: |
| LLaVA-NeXT-Vicuna-7B |               256 |         1e-3 |      1 |       4096 |            0 |

2. Finetuning


| Hyperparameter       | Global Batch Size | LLM lr | Projector lr | Vision Tower lr | Epochs | Max length | Weight decay |
| -------------------- | ----------------: | -----: | -----------: | --------------: | -----: | ---------: | -----------: |
| LLaVA-NeXT-Vicuna-7B |               128 |   2e-5 |         2e-5 |            2e-6 |      1 |       4096 |            0 |

### é¢„è®­ç»ƒ (ç‰¹å¾å¯¹é½)

```bash
bash scripts/next/vicuna-7b/pretrain.sh 
```

| Model                | Data           | Hardware      | Training Time  |
| -------------------- | -------------: | ------------: | -------------: |
| LLaVA-NeXT-Vicuna-7B | LLaVA-Pretrain | 8x H100 (80G) | ~ 4   hours    |
| LLaVA-NeXT-LLaMA3-8B | LLaVA-Pretrain | 8x H100 (80G) | ~ 4.5 hours    |

### è§†è§‰æŒ‡ä»¤å¾®è°ƒ

```bash
bash scripts/next/vicuna-7b/finetune.sh
```

| Model                | Data                  | Hardware      | Training Time  |
| -------------------- | --------------------: | ------------: | -------------: |
| LLaVA-NeXT-Vicuna-7B | LLaVA-v1.5-mix665K    | 8x H100 (80G) | ~ 11.5 hours   |
| LLaVA-NeXT-Vicuna-7B | Open-LLaVA-NeXT-mix1M | 8x H100 (80G) | ~ 17.5 hours   |
| LLaVA-NeXT-LLaMA3-8B | Open-LLaVA-NeXT-mix1M | 8x H100 (80G) | ~ 24.5 hours   |

### è½¬æ¢æ¨¡å‹æƒé‡

å¦‚æœä½ æƒ³è¦é€šè¿‡ ğŸ¤— Transformers åº“æ¥åŠ è½½è¿™ä¸ªæƒé‡ (å¦‚æ›´æ–¹ä¾¿åœ°ç”Ÿæˆå›å¤æˆ–é€šè¿‡ [open-compass/VLMEvalKit](https://github.com/open-compass/VLMEvalKit) æ¥æµ‹è¯•æ¨¡å‹)ï¼Œä½ éœ€è¦è½¬æ¢æ¨¡å‹æƒé‡ä¸º hf ç‰ˆæœ¬ã€‚å› ä¸ºè®­ç»ƒåä¿å­˜çš„æ¨¡å‹æƒé‡åªèƒ½ç”±å½“å‰è¿™ä¸ªä»“åº“çš„ä»£ç åŠ è½½ï¼Œè€Œä¸èƒ½é€šè¿‡ ğŸ¤— Transformers åº“æ¥åŠ è½½ã€‚

```bash
bash convert_llava_next_weights_to_hf.sh
```

## ğŸ™ è‡´è°¢

- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT)
- [Open-LLaVA-NeXT](https://github.com/xiaoachen98/Open-LLaVA-NeXT)
